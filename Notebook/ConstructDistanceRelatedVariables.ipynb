{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "from tqdm.notebook import tqdm  # ÊîπÁî®notebookÊ†∑Âºè\n",
    "\n",
    "from Constants import Constants as const\n",
    "\n",
    "tqdm.pandas(desc=\"Processing...\")  # ÈíàÂØπnotebookÁöÑÂàùÂßãÂåñ"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Try to append address information",
   "id": "44d6d1153c2d7d7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reg_df = pd.read_stata(os.path.join(const.RESULT_PATH, '20250603_stock_act_reg_data_v2.dta'))\n",
    "bill_header_df = pd.read_csv(os.path.join(const.DATABASE_PATH, 'bill mcdonald', 'LoughranMcDonald_10-K_HeaderData_1993-2024.zip'))"
   ],
   "id": "1bf5a7f8c678d204",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bill_header_df.head()",
   "id": "b2608bd0f50abd1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "reg_df['cik'] = reg_df['cik'].replace('', np.nan)",
   "id": "ccf9c58983433373",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ctat_df = pd.read_csv(os.path.join(const.COMPUSTAT_PATH, '1985_2024_ctat_firm_names.zip'),\n",
    "                      usecols=[const.GVKEY, const.CIK, 'fyear'])\n",
    "ctat_df = ctat_df.loc[ctat_df['fyear'] <= 2015].dropna(how='any').drop_duplicates(\n",
    "    subset=[const.GVKEY], keep='last').drop(['fyear'], axis=1)\n"
   ],
   "id": "cb676d88bdbb89ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reg_df2 = reg_df.merge(ctat_df, how='left', on=[const.GVKEY], suffixes=('', '_ctat'))\n",
    "reg_df2[const.CIK] = reg_df2[const.CIK + '_ctat'].fillna(reg_df2[const.CIK])\n",
    "reg_df2.drop([const.CIK + '_ctat'], axis=1, inplace=True)"
   ],
   "id": "85e10f66b0536b3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Âä†ËΩΩÊï∞ÊçÆ\n",
    "ctat_df = pd.read_csv(os.path.join(const.COMPUSTAT_PATH, '1950_2024_ctat_firm_location_information.zip'),\n",
    "                      usecols=[const.GVKEY, 'datadate', 'fyear', 'add1', 'add2', 'add3', 'add4', 'addzip', 'city', 'county', 'state'])\n",
    "\n",
    "# 1Ô∏è‚É£ Áî® datadate Ë°•ÂÖ® fyearÔºàÂ∞è‰∫é7ÊúàÂáè‰∏ÄÔºåÂ§ß‰∫éÁ≠â‰∫é7Êúà‰∏çÂèòÔºâ\n",
    "ctat_df['datadate_parsed'] = pd.to_datetime(ctat_df['datadate'], errors='coerce')\n",
    "\n",
    "# Âè™Âú® fyear ‰∏∫Á©∫ÁöÑË°å‰∏äËµãÂÄº\n",
    "mask_missing_fyear = ctat_df['fyear'].isna()\n",
    "ctat_df.loc[mask_missing_fyear, 'fyear'] = np.where(\n",
    "    ctat_df.loc[mask_missing_fyear, 'datadate_parsed'].dt.month < 7,\n",
    "    ctat_df.loc[mask_missing_fyear, 'datadate_parsed'].dt.year - 1,\n",
    "    ctat_df.loc[mask_missing_fyear, 'datadate_parsed'].dt.year\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ ÂéªÊéâ add1, add2, add3, add4, addzip, city, state ÂÖ®ÈÉ®‰∏∫Á©∫ÁöÑË°å\n",
    "address_cols = ['add1', 'add2', 'add3', 'add4', 'addzip', 'city', 'state']\n",
    "ctat_df = ctat_df.dropna(subset=address_cols, how='all')\n",
    "\n",
    "# 3Ô∏è‚É£ ÊØèÂπ¥ÊØèÂÖ¨Âè∏Âè™‰øùÁïô datadate ÊúÄÂ§ßÁöÑÈÇ£‰∏ÄË°å\n",
    "ctat_df = (ctat_df.sort_values('datadate_parsed')\n",
    "                    .groupby([const.GVKEY, 'fyear'], as_index=False)\n",
    "                    .tail(1))\n",
    "\n",
    "# ÊúÄÁªàËæìÂá∫ÔºàÂéªÊéâ datadate_parsed ‰∏¥Êó∂ÂàóÔºâ\n",
    "ctat_df = ctat_df.drop(columns=['datadate_parsed'])\n",
    "\n",
    "print(f\"‚úÖ Ê∏ÖÁêÜÂÆåÊàêÔºöÂâ©‰Ωô {len(ctat_df)} Ë°åÔºå{ctat_df[const.GVKEY].nunique()} ÂÆ∂ÂÖ¨Âè∏\")\n"
   ],
   "id": "8ec8482d0d49a6cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ctat_df.head()",
   "id": "5c4982336714760e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1Ô∏è‚É£ Á°Æ‰øù key Á±ªÂûã‰∏ÄËá¥\n",
    "ctat_df['gvkey'] = ctat_df['gvkey'].astype(int)\n",
    "reg_df2['gvkey'] = reg_df2['gvkey'].astype(int)\n",
    "ctat_df['fyear'] = ctat_df['fyear'].astype(int)\n",
    "reg_df2['fiscal_year'] = reg_df2['fiscal_year'].astype(int)\n",
    "\n",
    "ctat_df['datadate_parsed'] = pd.to_datetime(ctat_df['datadate'], errors='coerce')\n",
    "\n",
    "# 2Ô∏è‚É£ Êåâ gvkey + fyear ÂêàÂπ∂ ctat Âú∞ÂùÄ‰ø°ÊÅØÂà∞ reg_df\n",
    "merged_df = pd.merge(reg_df2, ctat_df, how='left', left_on=['gvkey', 'fiscal_year'], right_on=['gvkey', 'fyear'],\n",
    "                     suffixes=('', '_ctat'))\n",
    "\n",
    "# 3Ô∏è‚É£ ÂØπ‰∫éÂêå‰∏ÄÂπ¥Áº∫Â§±Âú∞ÂùÄÁöÑÔºåÁî®Êó∂Èó¥ÊúÄËøëÁöÑÂú∞ÂùÄË°•ÂÖ®\n",
    "\n",
    "# ÂÖàÈÄâÂá∫ ctat_df ‰∏≠ÊØè‰∏™ gvkey ÊúÄËøëÁöÑ‰∏ÄÊù°ËÆ∞ÂΩï\n",
    "latest_ctat = (ctat_df.sort_values('datadate_parsed', ascending=False)\n",
    "                        .groupby('gvkey', as_index=False)\n",
    "                        .first())\n",
    "\n",
    "# ÂÆö‰πâÂú∞ÂùÄÂ≠óÊÆµÂàóË°®ÔºàË¶ÅË°•ÂÖ®ÁöÑÂàóÔºâ\n",
    "address_cols = ['add1', 'add2', 'add3', 'add4', 'addzip', 'city', 'state']\n",
    "\n",
    "# ÈÅçÂéÜÊØè‰∏™Â≠óÊÆµÔºåÂ¶ÇÊûúÁº∫Â§±ÔºàNaNÔºâÔºåÁî® latest_ctat Ë°•ÂÖ®\n",
    "for col in address_cols:\n",
    "    merged_df[col] = merged_df[col].fillna(\n",
    "        merged_df.merge(latest_ctat[['gvkey', col]], on='gvkey', how='left')[f'{col}_y']\n",
    "    )\n",
    "\n",
    "# 4Ô∏è‚É£ ÂéªÊéâÂ§ö‰ΩôÂàóÔºàÊØîÂ¶Ç fyear_ctatÔºâ\n",
    "if 'fyear_ctat' in merged_df.columns:\n",
    "    merged_df = merged_df.drop(columns=['fyear_ctat', 'datadate_parsed', 'county', 'datadate'])\n",
    "\n",
    "print(f\"‚úÖ ÂêàÂπ∂ÂíåË°•ÂÖ®ÂÆåÊàêÔºåÊÄªË°åÊï∞Ôºö{len(merged_df)}\")\n"
   ],
   "id": "50979fd50854f07d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged_df.to_pickle(os.path.join(const.TEMP_PATH, '20250603_stock_act_address_data.pkl'))",
   "id": "b6e64e4890d8df89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# ÈÄâÊã©ÂîØ‰∏ÄÂú∞ÂùÄ\n",
    "address_cols = ['add1', 'add2', 'add3', 'add4', 'city_ctat', 'state', 'addzip']\n",
    "unique_addresses = merged_df[address_cols].drop_duplicates()\n",
    "print('Unique addresses number: ' + str(unique_addresses.shape[0]))\n",
    "\n",
    "# Â∞Ü NaN ÊõøÊç¢‰∏∫Á©∫Â≠óÁ¨¶‰∏≤ÔºåÂπ∂ÊãºÊé•Âú∞ÂùÄ\n",
    "unique_addresses['full_address'] = unique_addresses[address_cols].fillna('').apply(\n",
    "    lambda row: ' '.join(row.values.astype(str)), axis=1\n",
    ")\n",
    "\n",
    "# ÂàùÂßãÂåñ geocoder\n",
    "geolocator = Nominatim(user_agent=\"my_unique_app_2027\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)  # Âä†ÈÄüÁéáÈôêÂà∂Èò≤Â∞ÅIP\n",
    "\n",
    "# ÂÅáËÆæ unique_addresses ÊòØ‰Ω†Êï¥ÁêÜÂ•ΩÁöÑ DataFrameÔºåÂê´ 'full_address' Âàó\n",
    "results = []\n",
    "for address in tqdm(unique_addresses['full_address'].head(10)):\n",
    "    try:\n",
    "        location = geocode(address)\n",
    "        if location:\n",
    "            results.append((address, location.latitude, location.longitude))\n",
    "        else:\n",
    "            results.append((address, None, None))\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {address}: {e}\")\n",
    "        results.append((address, None, None))\n",
    "\n",
    "# ËΩ¨Êç¢‰∏∫ DataFrame\n",
    "geo_df = pd.DataFrame(results, columns=['full_address', 'latitude', 'longitude'])"
   ],
   "id": "34c010d9ce3b75da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# ========== ÂàùÂßãÂåñ ==========\n",
    "address_cols = ['add1', 'add2', 'add3', 'add4', 'city_ctat', 'state', 'addzip']\n",
    "unique_addresses = merged_df[address_cols].drop_duplicates()\n",
    "print('Unique addresses number: ' + str(unique_addresses.shape[0]))\n",
    "\n",
    "# ÊãºÊé•ÂÆåÊï¥Âú∞ÂùÄ\n",
    "unique_addresses['full_address'] = unique_addresses[address_cols].fillna('').apply(\n",
    "    lambda row: ' '.join(row.values.astype(str)), axis=1\n",
    ")\n",
    "\n",
    "# ÈúÄË¶ÅÂ§ÑÁêÜÁöÑÂú∞ÂùÄÂàóË°®\n",
    "remaining = set(unique_addresses['full_address'])\n",
    "results = {}\n",
    "\n",
    "# ‰∏ÄÊâπ user_agent ÂèØ‰ª•Âæ™ÁéØ‰ΩøÁî®ÔºàÊç¢ IP„ÄÅÊç¢Â§¥Áî®Ôºâ\n",
    "user_agents = [f\"wya_address_app_attempt_{i}\" for i in range(1, 11)]\n",
    "\n",
    "# ========== Âæ™ÁéØÈáçËØï ==========\n",
    "max_retries = 10\n",
    "for attempt in range(max_retries):\n",
    "    if not remaining:\n",
    "        print(\"‚úÖ All addresses successfully geocoded!\")\n",
    "        break\n",
    "\n",
    "    print(f\"üîÑ Attempt {attempt + 1} with user_agent {user_agents[attempt % len(user_agents)]}\")\n",
    "    geolocator = Nominatim(user_agent=user_agents[attempt % len(user_agents)])\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=2)\n",
    "\n",
    "    # ÈÄê‰∏™ËØ∑Ê±ÇÂâ©‰ΩôÁöÑÂú∞ÂùÄ\n",
    "    failed_this_round = set()\n",
    "    for address in tqdm(remaining):\n",
    "        try:\n",
    "            location = geocode(address)\n",
    "            if location:\n",
    "                results[address] = (location.latitude, location.longitude)\n",
    "            else:\n",
    "                failed_this_round.add(address)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {address}: {e}\")\n",
    "            failed_this_round.add(address)\n",
    "\n",
    "    # Êõ¥Êñ∞Ââ©‰ΩôÈúÄË¶ÅÈáçËØïÁöÑ\n",
    "    remaining = failed_this_round\n",
    "\n",
    "    if remaining:\n",
    "        print(f\"‚è≥ Waiting 10 minutes before next retry (still {len(remaining)} addresses left)...\")\n",
    "        time.sleep(10 * 60)  # 10 minutes in seconds\n",
    "\n",
    "# ========== ‰øùÂ≠òÁªìÊûú ==========\n",
    "# ËΩ¨Êç¢‰∏∫ DataFrame\n",
    "geo_df = pd.DataFrame.from_dict(results, orient='index', columns=['latitude', 'longitude']).reset_index()\n",
    "geo_df = geo_df.rename(columns={'index': 'full_address'})\n"
   ],
   "id": "dd850303de7af84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "remaining",
   "id": "14b286d5895c925a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "geolocator = Nominatim(user_agent='wya_test_2077')\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=2)\n",
    "geocode('#245, 7 West 41st Avenue, San Mateo, CA, 94403')"
   ],
   "id": "1042b85b71df8223",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "# ========== Your input ==========\n",
    "# remaining_addresses = list of full address strings you want to geocode\n",
    "# For example:\n",
    "# remaining_addresses = ['1600 Pennsylvania Ave NW, Washington, DC 20500', 'New York, NY 10001', ...]\n",
    "# Here we assume you already have that list.\n",
    "# You can load it from CSV if needed:\n",
    "# remaining_addresses = pd.read_csv(\"failed_addresses.csv\")['full_address'].tolist()\n",
    "\n",
    "# Insert your Mapbox token here\n",
    "MAPBOX_ACCESS_TOKEN = \"pk.eyJ1IjoibWFrcndhbmciLCJhIjoiY2pjbzl5M3NzMTN3djJ4bnhhM3h5NTN6ZCJ9.jiuTGT54fo2t1VNz70MNLw\"\n",
    "\n",
    "# ========== Geocode Function ==========\n",
    "def geocode_mapbox(address, access_token):\n",
    "    base_url = \"https://api.mapbox.com/geocoding/v5/mapbox.places/\"\n",
    "    url = f\"{base_url}{requests.utils.quote(address)}.json\"\n",
    "    params = {\n",
    "        \"access_token\": access_token,\n",
    "        \"limit\": 1,\n",
    "        \"country\": \"US\"  # Optional: restrict to US\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data['features']:\n",
    "            coords = data['features'][0]['geometry']['coordinates']\n",
    "            return coords[1], coords[0]  # lat, lon\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error for address: {address}, {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ========== Run Geocoding ==========\n",
    "for address in tqdm(remaining):\n",
    "    lat, lon = geocode_mapbox(address, MAPBOX_ACCESS_TOKEN)\n",
    "    results[address] = (lat, lon)\n",
    "    time.sleep(0.5)  # Slight delay to respect Mapbox rate limits (600 req/minute for free tier)\n"
   ],
   "id": "6ce500cb2c35282f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ËΩ¨Êç¢‰∏∫ DataFrame\n",
    "geo_df = pd.DataFrame.from_dict(results, orient='index', columns=['latitude', 'longitude']).reset_index()\n",
    "geo_df = geo_df.rename(columns={'index': 'full_address'})"
   ],
   "id": "cef64b2d814956c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "geo_df.loc[geo_df['latitude'].isnull()]",
   "id": "9b40fd4a482e243b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "geo_df.to_excel(os.path.join(const.TEMP_PATH, '20250603_stock_act_address_data.xlsx'), index=False)",
   "id": "34d749db6059baa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "geo_df_clear = pd.read_excel(os.path.join(const.TEMP_PATH, '20250603_stock_act_address_data.xlsx'))",
   "id": "87b0fb411ef2ceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique_addresses_with_geocode = unique_addresses.merge(geo_df_clear, on='full_address')\n",
    "reg_df3 = merged_df.merge(unique_addresses_with_geocode, on=address_cols, how='left')\n"
   ],
   "id": "d2a82e22badb7dcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "reg_df3.loc[reg_df3['latitude'].isnull()]",
   "id": "177da291bf8c8cbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "# ‚úÖ Washington D.C. ÁöÑÂú∞ÁêÜÂùêÊ†á\n",
    "dc_coords = (38.89511, -77.03637)  # Á∫¨Â∫¶, ÁªèÂ∫¶\n",
    "\n",
    "\n",
    "# ‚úÖ ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÂàóÔºåËÆ°ÁÆóË∑ùÁ¶ªÔºàÂçï‰ΩçÔºöÂÖ¨ÈáåÔºâ\n",
    "reg_df3['distance_to_dc_km'] = reg_df3.apply(\n",
    "    lambda row: geodesic((row['latitude'], row['longitude']), dc_coords).kilometers\n",
    "    if pd.notnull(row['latitude']) and pd.notnull(row['longitude']) else None,\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ],
   "id": "522aa9b4be462ff5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "reg_df3['distance_to_dc_km'].describe()",
   "id": "7fdeb3f9e1fd14e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reg_df3['cik'] = reg_df3['cik'].astype(float)\n",
    "reg_df3.drop(['add3', 'add4'], axis=1).to_stata(os.path.join(const.RESULT_PATH, '20250604_stock_act_reg_data_v1.dta'), write_index=False, version=119)"
   ],
   "id": "1a19d6c3345968d8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
